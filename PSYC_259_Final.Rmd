---
title: "PSYC 259 Final"
author: "Hunter B Sturgill"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: True
    toc_float: True
    code_folding: hide
    
    

---

```{r setup, include=FALSE}
library(tidyverse)
library(DataExplorer)

```
# Reproducibility 
In this section I will show steps I have taken to make my work easier to share
and increase its reproducibility.

##  Open Source 
I have create an OSF repository to help increase transparency and fidelity of 
my project. 

One of the major issues with my previous technique was that I had all raw data 
stored as a matrix of numbers. This matrix was accompanied by an extra document 
that was required to understand what each column represented with respect to the
experiment. To fix this I wrote a short MatLab program that converts the raw data matrix
into a table with column headers that should prove easy to understand without 
requiring an extra document or excessive commenting in the code.Additionally, this 
code shows good use of for loop to reduce the possibility of error in copying and 
pasting many times over. 

I also discovered how to read in data from a cloud storage which I did not know how
to do until taking this course. This will greatly reduce my chances of error when shuffling 
data from place to place and will allow me to be able to write programs that are more easily shared. 
For example previous data analysis programs have required multiple `data_home` variables for each of the collaborators on the project 

An example from a previous experiment below. 
```
ProgramHome=['C:\Users\hunter\Documents\Pacing Experiment'];
cd(ProgramHome)
% DataHome=['C:\Users\David\Documents\Pacing Experiment\Prelim data\'];
% DataHome=['C:\Users\hunter\Documents\Pacing Experiment\Natural Pace'];
%  DataHome=["C:\Users\Iman\Documents\Pacing Experiment\Data"]; % Experiment 2
DataHome=["C:\Users\User\Documents\Pacing Experiment\Data Experiment 3\All_Data"] %Experiment 3
cd(DataHome)
```
Switching directories can be eliminated if each of us are using the same repository instead of just downloading all of the data 


Below is code for reading in the data for cleaning

```
data_home= ("G:\Shared drives\Haptic Search Group Winter 2022\Experiment 2 Data_NVP")
program_home=( "C:\Users\hunte\OneDrive\Documents\MATLAB" )
Data=[];
Data_table=[];
DS_clean=[];
max_subjects=100;
num_of_sub= 0;
cd(data_home)

for si=1:max_subjects
    if exist(['Tactile_Search_2109_Experiment_2_Subject_',num2str(si),'_Block_2.csv'])
        num_of_sub= num_of_sub +1
        Data=[Data; csvread(['Tactile_Search_2109_Experiment_2_Subject_',num2str(si),'_Block_2.csv'])];
    end
end
cd(program_home);

```

This code also uses and iterative procedure to clean the data file by removing the missing
values that are generated. The program that collects this data fills in a preallocated matrix
as trials are completed and becasue we only want to analized half of the trials the matrix contains many unfilled rows. When combining each of those files you wind up with huge gaps of 
data that are all 0. 

```
for gi = 1:size(Data,1)
    if Data(gi,1) ~= 0 
    DS_clean= [DS_clean; Data(gi,:)];
    end
end
```
Now that the files are compiled and cleaned I want to save the new data as a table. 
Below is the code for that. 

```
titles= {'Year','Month','Day','Hour','Minute','Seconds','Subject','Experiment','Trial','Distractor_num','Distractor_size',...
    'Setup_time(s)','Inspection_time(s)','Search_time(s)','Input_time(s)','Accuracy'}
Data_table = array2table(DS_clean,"VariableNames",titles)
writetable(Data_table,'NVP_Clean_Data.csv')

```
The entire program can be found on my OSF page for this project [Haptic Search](https://osf.io/t23g4/?view_only=8540c0350e67422a9d49afd39c901c63)
in the programs folder. 


##  Cleaning up

One of the major issues I have noticed with my work is poor folder and file organization. 
There is a substatial amount of time wasted in attempting to find a program, some data, or 
the most recent version of a manuscript. I have take some steps to mitegate this issue 
by reorganizing my folder and also including the manuscripts in my OSF repository. 

Below is a screen shot of my folder for this project.

![***Before Cleaning***](Images\Haptic_Search_Folder_before.jpg)



![***During Cleaning*** ](Images\Haptic_Search_Folder_During.jpg)

 

![***After Cleaning***](Images\Haptic_Search_Folder_after.jpg)

As can be seen from the images I have added several new folders with specific 
naming conventions. I have purposefully left the important programs in the upstream 
folder in order to keep the directory calling down stream of the programs. 


##  Old Habits Die Hard 

In working on this project I realized I was making changes to my program and saving them with my usual method of incrementing the version number and saving it as a new file. It has occurred to me that this was completely unnecessary as both github and OSF will keep up with the changes between the two file even when they have the exact same name. 
I hope that this can be seen in the github repository where I cleaned up and added some comments to the `Cleaning_Data_for_Sharing` program where I saved another version after the change. I was shocking to me how quickly I did this and without consideration.
I think this is perhaps a habit that will be hard to shake but I now see how truly beneficial these repositories / programs really are. 


##  Exploratory Data Analysis 

In this section I will show some of the things I have learned throughout the course.
Particularly I was very poor at programming in r. 
I will read in my newly reformatted data and then plot some graphs that I believe will 
best show the results of the experiment. 



Read In no visual preview experiment data that has previously been cleaned. 
cleaning and reformatting was shown above in section [Open Source](#open-source)

```{r}
ds <-  read_csv("Cleaning Data for Sharing/NVP_Clean_Data.csv")
glimpse(ds)
```

Now as part of exploratory analysis I want to count up unique participant Id's 

```{r}
length(unique(ds$Subject))

```


Let's make a quick scatter plot to see what the data look like. 

```{r}
ggplot(ds, aes(x = Distractor_size, y = `Search_time(s)`)) + 
  geom_point()

```





