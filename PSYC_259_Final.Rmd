---
title: "PSYC 259 Final"
author: "Hunter B Sturgill"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: True
    toc_float: True
    code_folding: hide
    
    

---

```{r setup, include=FALSE}
library(tidyverse)
library(DataExplorer)
library(knitr)
```
# Reproducibility 
This is the primary concern in this Final Project. I have put considerable effort into
cleaning up my data for sharing and creating an OSF repository that I will be able to use
throughout the course of this project. 
It will be beneficial to my career as a graduate student as this Haptic Search project
is my primary study of interest and will be the work I plan to build my career on. 
Having an open source repository will lay a good foundation for sharing my work with reviewers and other scientist who are interested. 

Although the steps taken for this final project will not be all of the cleaning and 
organizing I will need to do, it is my hope that this will be the start of a better, more efficient way of working.

In the sections ahead, I will highlight some of the steps I have taken to cut down on mistakes,
improve efficiency by using repositories, and continue to develop file organization and naming conventions that are better suited for collaboration with others. 

In addition to these changes, I have briefly described learning that took place en vivo during the course of working on this project. It's not my belief that this adds to the project in any way but instead highlights a breakthrough that took place for me while working on this project and exemplifies the usefulness of a final project of this format. 
Here's to you both for a life changed by your efforts in this course! 

##  Open Source 
I have create an OSF repository to help increase transparency and fidelity of 
my project. 

One of the major issues with my previous technique was that I had all raw data 
stored as a matrix of numbers. This matrix was accompanied by an extra document 
that was required to understand what each column represented with respect to the
experiment. To fix this I wrote a short MatLab program that converts the raw data matrix
into a table with column headers that should prove easy to understand without 
requiring an extra document or excessive commenting in the code.Additionally, this 
code shows good use of the for loop to reduce the possibility of error in copying and 
pasting many times over. 

I also discovered how to read in data from cloud storage, which I did not know how
to do until taking this course. This will greatly reduce my chances of error when shuffling 
data from place to place and will allow me to be able to write programs that are easily shared. 
For example previous data analysis programs have required multiple `data_home` variables for each of the collaborators on the project. It sources the location of the data on each machine. This will no longer be necessary on future work. 

An example from a previous experiment below. 
```
ProgramHome=['C:\Users\hunter\Documents\Pacing Experiment'];
cd(ProgramHome)
% DataHome=['C:\Users\David\Documents\Pacing Experiment\Prelim data\'];
% DataHome=['C:\Users\hunter\Documents\Pacing Experiment\Natural Pace'];
%  DataHome=["C:\Users\Iman\Documents\Pacing Experiment\Data"]; % Experiment 2
DataHome=["C:\Users\User\Documents\Pacing Experiment\Data Experiment 3\All_Data"] %Experiment 3
cd(DataHome)
```
Switching directories can be eliminated if each of us are using the same repository instead of just downloading all of the data 


Below is code for reading in the data for cleaning. Note that the variable `data_home` pulls from a google drive where data was stored and anyone who uses this program need not create a new directory to find the data. 
*Of course there are some prior steps that need to be taken but are omitted because they do not contribute to the purposes of this project.* 

```
data_home= ("G:\Shared drives\Haptic Search Group Winter 2022\Experiment 2 Data_NVP")
program_home=( "C:\Users\hunte\OneDrive\Documents\MATLAB" )
Data=[];
Data_table=[];
DS_clean=[];
max_subjects=100;
num_of_sub= 0;
cd(data_home)

for si=1:max_subjects
    if exist(['Tactile_Search_2109_Experiment_2_Subject_',num2str(si),'_Block_2.csv'])
        num_of_sub= num_of_sub +1
        Data=[Data; csvread(['Tactile_Search_2109_Experiment_2_Subject_',num2str(si),'_Block_2.csv'])];
    end
end
cd(program_home);

```

This code also uses and iterative procedure to clean the data file by removing the missing
values that are generated. The program that collects this data fills in a pre-allocated matrix
as trials are completed.The Matrix is originally filled with all 0's and then saves after each trial is completed. Since I only analyzed half of the data there are large groups of 0's in the file that compiled each subjects' data. The code above read in all 36 trials for each subject and put them all together but 18 of those 36 trials had yet to be completed. Thus the final data set needs to be cleaned of those 18 rows of 0's for each subject. 

Raw data can be found at [OSF](https://osf.io/t23g4/?view_only=8540c0350e67422a9d49afd39c901c63) under `Experiments`,
`NVP_Experiment`, `Data`, `Raw Data Files`.

Below is the code to clean out the extra 0's

```
for gi = 1:size(Data,1)
    if Data(gi,1) ~= 0 
    DS_clean= [DS_clean; Data(gi,:)];
    end
end
```
Now that the files are compiled and cleaned I want to save the new data as a table. 
Below is the code for that.Each column is given a descriptive header. I chose to save the file as a .csv as I feel it is the most universal format and can be used by many analysis programs. 

```
titles= {'Year','Month','Day','Hour','Minute','Seconds','Subject','Experiment','Trial','Distractor_num','Distractor_size',...
    'Setup_time(s)','Inspection_time(s)','Search_time(s)','Input_time(s)','Accuracy'}
Data_table = array2table(DS_clean,"VariableNames",titles)
writetable(Data_table,'NVP_Clean_Data.csv')

```
The entire program can be found on my OSF page for this project [Haptic Search](https://osf.io/t23g4/?view_only=8540c0350e67422a9d49afd39c901c63)
in the `programs` folder. 


##  Cleaning up

One of the major issues I have noticed with my work is poor folder and file organization. 
There is a substantial amount of time wasted in attempting to find a program, some data, or 
the most recent version of a manuscript. I have take some steps to mitigate this issue 
by reorganizing my folder and also including the most important items in my OSF repository. 

Below is a screen shot of my folder for this project, on my personal computer.

![***Before Cleaning***](Images\Haptic_Search_Folder_before.jpg)



![***During Cleaning*** ](Images\Haptic_Search_Folder_During.jpg)

 

![***After Cleaning***](Images\Haptic_Search_Folder_after.jpg)

As can be seen from the images I have added several new folders with specific 
naming conventions. I have purposefully left the important programs in the upstream 
folder in order to keep the directory calling down stream of the programs. 


##  Old Habits Die Hard 

In working on this project I realized I was making changes to my program and saving them with my usual method of incrementing the version number and saving it as a new file. It has occurred to me that this was completely unnecessary as both github and OSF will keep up with the changes between the two file even when they have the exact same name. 
I hope that this can be seen in the github repository where I cleaned up and added some comments to the `Cleaning_Data_for_Sharing` program where I saved another version after the change. It was shocking to me how quickly I did this and without consideration.
I think this is perhaps a habit that will be hard to shake but I now see how truly beneficial these repositories / programs really are. 


##  Exploratory Data Analysis 

In this section I will show some of the things I have learned throughout the course.
Particularly, I was very poor at programming in r prior to taking this course, now I have both produced an html markdown file and a graph that could be used in future talks. I would not have considered doing either of these things 10 weeks ago. 


I will read in my newly reformatted data and then plot some graphs that I believe will 
best show the results of the experiment. 



Read In no visual preview experiment data that has previously been cleaned. 
cleaning and reformatting was shown above in section [Open Source](#open-source)


```{r warning=FALSE, message=FALSE}
ds <-  read_csv("Cleaning Data for Sharing/NVP_Clean_Data.csv")
glimpse(ds)
```

Now as part of exploratory analysis I want to count up unique participant Id's 

```{r message=FALSE}
length(unique(ds$Subject))

```


Let's make a quick scatter plot to see what the data look like. I want to plot search times as a function of distractor size.  

```{r warning=FALSE, message=FALSE}
ggplot(ds, aes(x = Distractor_size, y = `Search_time(s)`)) + 
  geom_point()

```

The previous graph is nice and give a good idea of the distribution of search times for each of the 6 different distractor lengths. However, this aggregates over the number of distractors in the search set. I would like to plot a line graph that will show aggregate data for each of the `r 3*6` conditions.  


Start by getting the means for each of the conditions. I will group by number of distractors first, so 4, 6, or 8. Then in each of those groups I should have a grouping by the length of the distractors[ .25" .5" .75' 1.25" 1.5" 1.75" ] where the target is always 1"

```{r message=FALSE}
# Get conditional means 
cond_mean <- ds %>%  group_by(Distractor_num, Distractor_size) %>% 
  summarise_at(vars(`Search_time(s)`), list(name = mean))
kable(cond_mean) 
```

Now I want to plot the aggregate data and have the parameter that distinguishes the curves be the number of distractors in the search. 

```{r}
# Plot conditional means

ggplot(cond_mean, aes(x=Distractor_size, y= name, color = Distractor_num))+
  geom_line(aes(group = Distractor_num)) + geom_point(size = 2) + scale_x_continuous(name="Distractor Size(inches)", breaks=seq(0,2,.25)) +
  scale_y_continuous(name="Search Time(s)", breaks=seq(0,40,5))
  

```

Now that I have a nice graph for my search times I want to produce a similar graph with the accuracy data. Considering that I will be making this graph again for the next experiment and perhaps many more I want to make a function that will produce this same graph for both accuracy and search time
I will start with an argument that asks for the data set, this will allow me to keep the experiments straight. 
Then function will call an argument that specifies which type of graph to create, 1 for accuracy and 2 for search times. I know this is essentially hard coding but for my skill level I will stick with this convention for now. 


Start with the function.

```{r message=FALSE,warning=FALSE}

Haptic_plots <- function(ds,DV){
  dep_var <- DV
  if (dep_var == 1) {
    cond_mean <- ds %>%  group_by(Distractor_num, Distractor_size) %>% 
    summarise_at(vars(Accuracy), list(name = mean))
    cond_mean %>% group_by(Distractor_num)
  
  p2 <-  ggplot(cond_mean, aes(x=Distractor_size, y= name, color = Distractor_num))+
  geom_line(aes(group = Distractor_num)) + geom_point(size = 2) +   scale_x_continuous(name="Distractor Size(inches)", breaks=seq(0,2,.25)) +
  scale_y_continuous(name="Accuracy", breaks=seq(.5,1,.05))
  return(p2)
  }
  else {
    cond_mean <- ds %>%  group_by(Distractor_num, Distractor_size) %>% 
    summarise_at(vars(`Search_time(s)`), list(name = mean))
  
    cond_mean %>% group_by(Distractor_num)
  
    p1 <- ggplot(cond_mean, aes(x=Distractor_size, y= name, color = Distractor_num))+
  geom_line(aes(group = Distractor_num)) + geom_point(size = 2) + scale_x_continuous(name="Distractor Size(inches)", breaks=seq(0,2,.25)) +
  scale_y_continuous(name="Search Time(s)", breaks=seq(0,40,5))
  
  return(p1)
  }
}


```

Now let's test the function 

Test accuracy first. 
```{r warning=FALSE,message=FALSE}
Haptic_plots(ds,1)

```


No again for search times. 

```{r warning=FALSE,message=FALSE}
Haptic_plots(ds,2)

```